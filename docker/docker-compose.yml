name: kri-local-rag
services:
  # ---------- RAG CLI -----------------------------------------
  cli:
    build:
      context: ..  # project root
      dockerfile: docker/CLI.Dockerfile
    depends_on:
      weaviate:
        condition: service_healthy
      ollama:
        condition: service_healthy
    environment:
      # Point the Python code to the other services inside the compose network
      - OLLAMA_URL=http://ollama:11434
      - WEAVIATE_URL=http://weaviate:8080
    volumes:
      - ../:/app
      - ../data:/app/data
      - hf_cache:/root/.cache/huggingface
    working_dir: /app
    # Keep container running for exec sessions
    command: ["sleep", "infinity"]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    secrets: [] # No secrets needed for the backend at the moment

  # ---------- Local Weaviate database ------------------------------------------------
  weaviate:
    image: cr.weaviate.io/semitechnologies/weaviate:1.25.1
    ports:
      - "8080:8080"
      - "50051:50051"
    volumes:
      - weaviate_db:/var/lib/weaviate
    environment:
      QUERY_DEFAULTS_LIMIT: 25
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'
      DEFAULT_VECTORIZER_MODULE: 'none' # We are providing vectors manually
      ENABLE_MODULES: 'text2vec-huggingface,reranker-huggingface' # Still needed for reranker
      CLUSTER_HOSTNAME: 'node1'
    healthcheck:
      test: ["CMD-SHELL", "wget -q --spider http://localhost:8080/v1/.well-known/ready || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  # ---------- Local LLM server for generating RAG answers ---------------------------------------------------
  ollama:
    image: ollama/ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    volumes:
      - ollama_models:/root/.ollama
    ports:
      - "11434:11434"
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 60s

  # ---------- Streamlit RAG App (contains backend and frontend) -----------------------------------------
  app:
    build:
      context: ..  # project root
      dockerfile: docker/app.Dockerfile
    depends_on:
      weaviate:
        condition: service_healthy
      ollama:
        condition: service_healthy
    ports:
      - "8501:8501"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    volumes:
      - hf_cache:/root/.cache/huggingface
    environment:
      # Point the Python code to the other services inside the compose network
      - OLLAMA_URL=http://ollama:11434
      - WEAVIATE_URL=http://weaviate:8080

  # ---------- One-off Ingestion Utility -----------------------------------------
  ingester:
    profiles: ["ingest"]
    image: kri-local-rag-cli   # reuse the lightweight CLI image
    depends_on:
      weaviate:
        condition: service_healthy
      ollama:
        condition: service_healthy
    volumes:
      # Mount the whole project so python files are always up-to-date (no rebuild needed)
      - ../:/app
      - ../data:/app/data
      # Persist HuggingFace cache between one-off runs to avoid repeated downloads
      - hf_cache:/root/.cache/huggingface
    command: ["python", "backend/ingest_pdf.py", "/app/data"]

  # ---------- GraphQL Playground -----------------------------------------
  graphql-playground:
    image: quay.io/graphql/graphql-playground   # public & tiny
    ports:
      - "8081:80"
    environment:
      # Tell the playground which endpoint to use for requests
      - GRAPHQL_ENDPOINT=http://weaviate:8080/v1/graphql

volumes:
  weaviate_db:
  ollama_models:
  hf_cache:
