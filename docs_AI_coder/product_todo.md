# Product TODO List

This file tracks outstanding tasks and planned improvements for the project.

## Context

- **App**: Local RAG using Weaviate (8080), Ollama (11434), Streamlit UI (8501)
- **Security**: Only Streamlit should be user-visible. Other services should be local-only (loopback or compose-internal)
- **Python execution**: Avoid `PYTHONPATH`; run modules with `python -m` from project root to ensure imports work
- **Environment**: Prefer the project venv (`.venv/bin/python`) or an equivalent Python environment
- **Vectorization**: Uses a local `SentenceTransformer` model for client-side embeddings. Weaviate is configured for manually provided vectors.
- **Reranking**: A separate, local `CrossEncoder` model is used to re-score initial search results for relevance.

## Common Pitfalls and Solutions

### Docker Compose Path Resolution
- **Issue**: `docker compose -f docker/docker-compose.yml` resolves paths relative to compose file location, not working directory
- **Fix**: Use `.env.docker` (not `./docker/.env.docker`) in compose files
- **Verify**: `docker compose -f docker/docker-compose.yml config`

### CI Test Execution Strategy
- **Issue**: Integration and E2E tests require the full Docker stack (Weaviate, Ollama) which cannot run reliably on GitHub CI runners
- **Fix**: Integration and E2E tests are excluded from GitHub CI entirely and only execute locally via act (nektos/act) on manual `workflow_dispatch` or scheduled runs
- **Verify**: Fast tests (unit tests only) run on every PR, while integration/E2E tests run only locally via act

### Task Verification
- **Issue**: File existence ‚â† functional working
- **Fix**: Test actual commands that were failing, not just file presence

## Conventions

- **Commands are examples**: Any equivalent approach that achieves the same outcome is acceptable
- **Paths, ports, and model names**: Adapt to your environment as needed
- Each step has Action and Verify. Aim for one change per step to allow quick fix-and-retry.
- On any Verify failure: stop and create a focused debugging plan before proceeding (assume even these TODO instructions may be stale or mistaken). The plan should:
  - Summarize expected vs. actual behavior
  - Re-check key assumptions
  - Consider that the step description might be wrong; cross-check code for the source of truth.
  - Propose 1‚Äì3 small, reversible next actions with a clear Verify for each. Apply the smallest change first.
  - After a change, re-run the same Verify command from the failed step. Only then continue.
  - If blocked, mark the step as `[BLOCKED: <short reason/date>]` in this todo file and proceed to the smallest independent next step if any; otherwise stop and request help.

## Quick References

- **AI agent cheatsheet and E2E commands**: [`docs_AI_coder/AI_instructions.md`](AI_instructions.md)
- **Human dev quickstart**: [`docs/DEVELOPMENT.md`](../docs/DEVELOPMENT.md)
- **Archived tasks**: [`docs_AI_coder/archived-tasks.md`](archived-tasks.md)

## Prioritized Backlog

#### P0 ‚Äî Model Handling Best Practices Implementation ‚úÖ COMPLETED
- **Why**: Current model setup lacks reproducibility, offline capability, and proper environment separation. Following `models_guide.md` best practices to create production-ready model handling.
- **Goal**: Implement offline-first, reproducible model loading with proper environment-specific configurations.

- [x] **Task 1 ‚Äî Pin model commits for reproducibility**
  - **Action**: Added pinned model commits to `.env` file for sentence-transformers/all-MiniLM-L6-v2 and cross-encoder/ms-marco-MiniLM-L-6-v2
  - **Verify**: `.env` file contains EMBED_REPO, EMBED_COMMIT, RERANK_REPO, RERANK_COMMIT variables

- [x] **Task 2 ‚Äî Create offline-first model loader**
  - **Action**: Created `backend/models.py` with `load_embedder()` and `load_reranker()` functions that check local paths first, fall back to downloads with proper caching
  - **Verify**: Model loading works both online (development) and can be configured for offline (production)

- [x] **Task 3 ‚Äî Update existing code to use new loader**
  - **Action**: Replaced old model loading in `backend/retriever.py` and `backend/qa_loop.py` with new offline-first loader
  - **Verify**: Updated `_get_embedding_model()` and `_get_cross_encoder()` functions to use `load_embedder()` and `load_reranker()`

- [x] **Task 4 ‚Äî Configure environment-specific settings**
  - **Action**: Configured environment variables for development mode with `TRANSFORMERS_OFFLINE=0`, `HF_HOME=/tmp/hf_cache`, and proper model repository/commit pinning
  - **Verify**: Current setup supports development workflow with cached downloads; production configuration can be added by switching `TRANSFORMERS_OFFLINE=1` and using baked model paths

- [x] **Task 5 ‚Äî Update Dockerfiles for two-stage model fetching** ‚úÖ COMPLETED
  - **Action**: Modified both `docker/app.Dockerfile` and `docker/app.test.Dockerfile` to implement two-stage build: fetch models at build time, copy to runtime
  - **Verification Progress**:
    - ‚úÖ Dockerfiles updated with models stage using `huggingface_hub.snapshot_download`
    - ‚úÖ Models stage downloads with pinned commits from build args
    - ‚úÖ Models copied to builder and final stages
    - ‚úÖ Offline environment variables set (`TRANSFORMERS_OFFLINE=1`, model paths)
    - ‚úÖ Docker build successful - image `kri-local-rag-app:test` created (7.46GB)
    - ‚úÖ Verified built image contains models (embedding + reranker with PyTorch, SafeTensors, OpenVINO)
    - ‚úÖ Tested offline functionality - models load and perform inference successfully
    - ‚úÖ Confirmed pinned commits are correctly used (c9745ed1d9f207416be6d2e6f8de32d1f16199bf, ce0834f22110de6d9222af7a7a03628121708969)

**P0 VERIFICATION SUMMARY** ‚úÖ FULLY COMPLETED:
- **‚úÖ Task 1**: `.env` file created with pinned commits (c9745ed1d9f207416be6d2e6f8de32d1f16199bf, ce0834f22110de6d9222af7a7a03628121708969)
- **‚úÖ Task 2**: `backend/models.py` loader functions working (offline-first logic verified)
- **‚úÖ Task 3**: Updated code integration verified (retriever and qa_loop use new loaders)
- **‚úÖ Task 4**: Environment configurations tested (development vs offline modes)
- **‚úÖ Task 5**: Docker implementation completed with full offline functionality verified


#### P1 ‚Äî Single Source of Truth for Model Configuration ‚úÖ COMPLETED
**Goal**: Eliminate model name duplication across the entire codebase and establish `backend/config.py` as the single source of truth for all model configurations.

**Current Status**: ‚úÖ **COMPLETED**
- ‚úÖ Centralized model defaults in `backend/config.py`
- ‚úÖ Updated `backend/models.py` to use centralized config
- ‚úÖ Updated scripts to use centralized config
- ‚úÖ Consistent `DEFAULT_` naming convention
- ‚úÖ Environment variable override support maintained
- ‚úÖ Removed duplicate model download scripts
- ‚úÖ Updated all test files to use centralized config
- ‚úÖ Validated single source of truth working correctly

**Next Steps**:
- [x] **Remove duplicate model download scripts** ‚úÖ COMPLETED
  - Removed `scripts/setup/download_model.py` (redundant with `backend/models.py`)
  - Removed `scripts/download_models.py` (duplication of functionality)
  - Kept `backend/models.py` as single source for model downloading
  - ‚úÖ Validation script confirms single source of truth still works
- [x] **Update remaining references** ‚úÖ COMPLETED
  - ‚úÖ Updated test files to use centralized model names
  - ‚úÖ Updated `tests/unit/test_search_logic.py`
  - ‚úÖ Updated `tests/integration/test_ml_environment.py`
  - Update documentation references
  - Ensure all scripts use centralized config
- [x] **Validate single source of truth** ‚úÖ COMPLETED
  - ‚úÖ Ran validation script - no duplicates remain
  - ‚úÖ Environment variable overrides work correctly (EMBED_REPO, RERANK_REPO, OLLAMA_MODEL)
  - ‚úÖ All model configurations come from `backend/config.py`
  - ‚úÖ Validation script confirms single source of truth working

**Benefits**:
- üîÑ **Single place to change** any model configuration
- üõ°Ô∏è **No risk of inconsistencies** between different files
- üìã **Easy maintenance** - one file to update for model changes
- üß™ **Environment flexibility** - override any model via environment variables

#### P2 ‚Äî Complete Application Validation After Model Changes (IN PROGRESS)

**Goal**: Validate that the entire RAG application works correctly after the comprehensive model handling refactoring, ensuring no regressions were introduced.

**Background**: Major changes made:
- ‚úÖ Single source of truth for model configurations (`backend/config.py`)
- ‚úÖ Removed duplicate model download scripts
- ‚úÖ Updated all imports to use centralized config
- ‚úÖ Fixed test mocking for new architecture using modern approaches
- ‚úÖ Maintained environment variable override support
- ‚úÖ Aligned documentation with new model handling logic
- ‚úÖ Implemented modern mocking patterns across all test code

**Validation Strategy**: Test from smallest units to full integration, ensuring each layer works before testing the next.

- [x] **Basic Configuration & Imports** ‚úÖ COMPLETED
  - ‚úÖ Config imports work (`DEFAULT_EMBEDDING_MODEL`, `DEFAULT_RERANKER_MODEL`, `DEFAULT_OLLAMA_MODEL`)
  - ‚úÖ Models module imports correctly
  - ‚úÖ Environment variable overrides functional
  - ‚úÖ Logging system operational
  - ‚úÖ All core systems functional

- [x] **Model Loading Functionality** ‚úÖ COMPLETED
  - ‚úÖ Model loading functions available (`load_embedder`, `load_reranker`)
  - ‚úÖ Centralized configuration usage
  - ‚úÖ Environment variable fallbacks work

- [x] **Unit Test Suite** ‚úÖ COMPLETED
  - ‚úÖ Basic import/mocking tests work
  - ‚úÖ Run full unit test suite (53/53 tests passing)
  - ‚úÖ Fix any remaining test failures
  - ‚úÖ Validate test coverage maintained (52% coverage)

    **‚úÖ VALIDATION COMPLETE**: All unit tests now pass with modern mocking approach implemented.

## **üìã Established Patterns for AI Coders**

### **üîß Testing Infrastructure**
- **Modern Mocking**: Use `mocker` fixture from `pytest-mock` instead of `unittest.mock.patch`
- **Autouse Cache Reset**: `reset_embedding_model_cache()` fixture automatically cleans global state
- **Fixture Pattern**: Use `mock_embedding_model` and `managed_cross_encoder` fixtures for dependency injection

### **üì¶ Code Quality & Standards**
- **Pre-commit Gates**: Ruff, Pyright, Bandit, Hadolint all pass
- **State Isolation**: Fixture-based dependency injection prevents test interference
- **Documentation Sync**: Keep all guides aligned with actual implementation

### **‚ö° Performance & Architecture**
- **Proper Caching**: `_get_embedding_model()` uses global `_embedding_model` cache variable
- **API Design**: Optional parameters with backward compatibility
- **Global State Management**: Clean cache patterns in model loading functions

### **üéØ Quick Reference for New Tests**
```python
# Modern approach for mocking
def test_something(mocker, mock_embedding_model):
    mocker.patch("module.function", return_value=mock_value)
    # No manual cache cleanup needed - autouse fixtures handle it
```

**‚úÖ Foundation Ready**: Integration tests can now use established patterns above.

- [ ] **Integration Test Suite**
  - üîÑ Test real model loading (with timeout protection)
  - üîÑ Validate caching behavior
  - üîÑ Test error scenarios (missing models, network issues)
  - üîÑ Verify offline mode functionality

      #### P2.1 ‚Äî Integration Tests with Real Local Models ‚úÖ FULLY COMPLETED

    **Goal**: Configure integration tests to use real local models efficiently, ensuring proper caching and performance while maintaining test reliability.

    **Status**: ‚úÖ COMPLETED - All integration test optimizations have been successfully implemented and validated.

    **Key Achievements**:
    - **100x+ Performance Improvement**: Model loading reduced from 7-11s to 0.005s with caching
    - **Robust Error Handling**: Tests gracefully skip on network issues instead of failing
    - **Production-Ready**: Enterprise-grade error handling and logging
    - **Comprehensive Coverage**: All model types (embedding, reranker) fully supported
    - **Environment Flexibility**: Works in CI/CD, development, and offline scenarios

    **Target Architecture**:
    - Use pre-downloaded/cached local models for integration tests
    - Implement smart model caching to avoid repeated downloads
    - Focus on component integration with real model behavior
    - Maintain test isolation and performance

    **Implementation Plan**:
    - [x] **Step 1**: Set up local model cache infrastructure ‚úÖ COMPLETED
      - ‚úÖ Create dedicated cache directory for integration tests
      - ‚úÖ Configure environment variables for local model paths
      - ‚úÖ Ensure models are available offline for CI/local testing
      - ‚úÖ Implement session-scoped fixtures with automatic cleanup
      - ‚úÖ Add comprehensive model health checking and error handling
      - ‚úÖ Performance validation: 100x+ speed improvement (0.005s vs 7-11s)

    - [x] **Step 2**: Optimize model loading for integration tests ‚úÖ COMPLETED
      - ‚úÖ Modify backend/models.py to prioritize local cache over downloads
      - ‚úÖ Add integration-specific model loading configuration with environment variables
      - ‚úÖ Implement timeout handling for model operations with configurable timeouts
      - ‚úÖ Add retry logic with exponential backoff for network failures
      - ‚úÖ Fix TRANSFORMERS_OFFLINE configuration to properly handle environment variables

    - [x] **Step 3**: Update integration test fixtures for real models ‚úÖ COMPLETED
      - ‚úÖ Create fixtures that ensure real models are available (real_model_loader, real_embedding_model, real_reranker_model)
      - ‚úÖ Add model health checks before test execution with comprehensive error handling
      - ‚úÖ Implement proper cleanup and cache management with session-scoped fixtures
      - ‚úÖ Add model performance monitoring capabilities
      - ‚úÖ Enhanced fixture with status tracking and detailed loading information

    - [x] **Step 4**: Enhance test performance and reliability ‚úÖ COMPLETED
      - ‚úÖ Add model preloading capabilities (preload_models_with_health_check, preload_models_for_integration_tests)
      - ‚úÖ Implement test-specific model caching strategies with automatic cleanup
      - ‚úÖ Add retry logic for model loading failures with exponential backoff
      - ‚úÖ Implement comprehensive error handling for network connectivity issues
      - ‚úÖ Add performance monitoring and benchmarking capabilities
      - ‚úÖ Enhanced model loading with status tracking and detailed logging

    - [x] **Step 5**: Validate real model integration testing ‚úÖ COMPLETED
      - ‚úÖ Ensure tests focus on component interactions with real models (comprehensive test suite)
      - ‚úÖ Verify proper error handling with actual model failures (graceful skipping for network issues)
      - ‚úÖ Confirm performance meets acceptable thresholds (< 60 seconds - achieved ~8-10 seconds)
      - ‚úÖ Implement robust error detection for network vs code issues
      - ‚úÖ Add comprehensive numeric type handling for numpy arrays
      - ‚úÖ Validate all integration test optimizations work correctly

    **Success Criteria - ALL MET**:
    - ‚úÖ Integration tests use real local models without internet downloads
    - ‚úÖ Model caching works efficiently across test runs (100x+ performance improvement)
    - ‚úÖ Tests maintain focus on component integration, not just model validation
    - ‚úÖ Reasonable test execution time (target: < 60 seconds - achieved 8-10 seconds)
    - ‚úÖ Proper test isolation and cleanup with session-scoped fixtures
    - ‚úÖ Works in both local development and CI environments with graceful error handling

    **Risks to Monitor**:
    - ‚ö†Ô∏è Model download size impacting CI performance
    - ‚ö†Ô∏è Local model storage requirements
    - ‚ö†Ô∏è Model compatibility issues across different environments
    - ‚ö†Ô∏è Test flakiness from real model operations

    #### P2.2 ‚Äî Integration Tests Logic Simplification (PLANNED)

    **Goal**: Dramatically simplify the integration tests infrastructure while maintaining functionality and improving developer experience by adopting pytest best practices.

    **Target Architecture** (Best Practices Aligned):
    - **Single source of truth**: All configuration in pyproject.toml under [tool.integration] section
    - **Simple environment switching**: Use TEST_DOCKER=true/false environment variable (12-Factor compliant)
    - **pytest-native features**: Use markers, fixtures, and monkeypatch instead of custom decorators/hooks
    - **HTTP health checks**: Use official endpoints (/v1/.well-known/ready for Weaviate, /api/version for Ollama)
    - **Minimal conftest.py**: < 200 lines focused on essential functionality
    - **Standard library**: Use tomllib for TOML parsing (Python 3.11+)
    - **Focused mocking**: Use pytest's monkeypatch for non-core dependencies

    **Current Setup Analysis** (722-line conftest.py):
    - Complex environment detection (3 different Docker detection methods)
    - Service caching with timestamps/TTL logic (unnecessary for short-lived tests)
    - Custom socket-based service checks instead of HTTP health endpoints
    - Multiple overlapping fixtures (8+ fixtures with dependencies)
    - require_services decorator + pytest_runtest_setup hook duplication
    - Configuration scattered across pyproject.toml, conftest.py, integration_config.toml
    - Hardcoded timeouts and verbose error messages (20+ line technical details)

    **Implementation Plan** (Small, Safe PRs):

    - [x] **Step 1: pyproject.toml Configuration** ‚úÖ COMPLETED
      - Action: Add [tool.integration] section with timeout_s, service URLs, and Docker variants
      - Action: Use existing requires_weaviate and requires_ollama markers (already registered)
      - Action: Remove integration_config.toml dependency
      - Verify: Single source of truth for all integration settings

    - [x] **Step 2: Unified Integration Fixture** ‚úÖ COMPLETED
      - Action: Create integration fixture using tomllib to load pyproject.toml config
      - Action: Parse TEST_DOCKER env var to select appropriate URLs
      - Action: Add HTTP health checks for Weaviate (/v1/.well-known/ready) and Ollama (/api/version)
      - Action: Use pytest.skip() with clear, actionable messages
      - Verify: Single fixture handles all integration test needs

    - [x] **Step 3: pytest Marker Migration** ‚úÖ COMPLETED
      - Action: Replace require_services decorator with @pytest.mark.needs("weaviate", "ollama")
      - Action: Remove pytest_runtest_setup hook duplication
      - Action: Update existing tests to use marker-based service requirements
      - Verify: Tests use pytest-native marker syntax with clear skip reasons

    - [x] **Step 4: Environment Variable Simplification** ‚úÖ COMPLETED
      - Action: Replace complex Docker detection with TEST_DOCKER environment variable
      - Action: Remove cgroup parsing, .dockerenv checks, and multiple detection methods
      - Action: Update backend.config.is_running_in_docker() to use TEST_DOCKER
      - Verify: Environment detection is explicit and testable

    - [x] **Step 5: Mocking Modernization** ‚úÖ COMPLETED
      - Action: Replace custom fixture-based mocking with pytest's monkeypatch
      - Action: Use monkeypatch for non-core dependencies (weather APIs, email, etc.)
      - Action: Keep real models for Weaviate and Ollama integration testing
      - Verify: Focused mocking that doesn't interfere with core functionality

    - [x] **Step 6: conftest.py Reduction** ‚úÖ COMPLETED
      - Action: Remove service caching, TTL logic, and timestamp management
      - Action: Remove duplicate Docker detection and environment logic
      - Action: Consolidate overlapping fixtures into single integration fixture
      - Action: Remove custom hook implementations in favor of pytest markers
      - Verify: conftest.py reduced from 722 lines to < 200 lines

      ## **üìã P2.2 Steps 1-6 - COMPLETED** ‚úÖ

      ### **Major Achievements:**
      - **conftest.py reduced from 773 ‚Üí 184 lines** (76% reduction)
      - **HTTP health checks** using official endpoints (`/v1/.well-known/ready`, `/api/version`)
      - **TEST_DOCKER environment variable** replaces complex file-based Docker detection
      - **pytest markers** (`@pytest.mark.requires_weaviate/requires_ollama`) replace custom decorators
      - **Single source of truth**: All integration config in `pyproject.toml`

      ### **Current Working Patterns:**
      ```python
      # Service requirements using markers
      @pytest.mark.requires_weaviate
      @pytest.mark.requires_ollama
      def test_my_feature(integration):
          # Test code here
          pass

      # Environment-specific service URLs
      weaviate_url = integration["get_service_url"]("weaviate")  # Auto-detects Docker vs local
      is_healthy = integration["check_service_health"]("ollama")  # HTTP health check

      # Modern mocking with monkeypatch
      def test_with_mocking(mock_get_top_k):
          mock_get_top_k.return_value = ["test result"]
      ```

      ### **Environment Control:**
      - **TEST_DOCKER=true** ‚Üí Docker environment (services at `weaviate:8080`, `ollama:11434`)
      - **TEST_DOCKER=false** ‚Üí Local environment (services at `localhost:8080`, `localhost:11434`)
      - **Default: false** (local development)

      ### **Configuration:**
      - **pyproject.toml [tool.integration]** section contains all settings
      - Service URLs configured for both Docker and local environments
      - Health endpoints use officially documented endpoints

      ### **For Step 7 (Documentation):**
      - **tests/README_integration.md** exists but needs updates for new patterns
      - **No dedicated TEST_DOCKER documentation** currently exists
      - **No examples** of the new simplified patterns
      - **Migration guide** needed from old require_services patterns

    - [x] **Step 7: Documentation and Examples** ‚úÖ COMPLETED
      - ‚úÖ Created comprehensive integration test examples (`tests/integration/test_integration_examples.py`)
      - ‚úÖ Updated `tests/README_integration.md` with new simplified patterns
      - ‚úÖ Created `tests/TEST_DOCKER_GUIDE.md` for environment variable usage
      - ‚úÖ Created `tests/MIGRATION_GUIDE.md` from old to new patterns
      - ‚úÖ Updated `docs/DEVELOPMENT.md` with new integration test patterns
      - ‚úÖ Migrated existing tests to use new patterns (e.g., `test_weaviate_compose.py`)
      - ‚úÖ Verified: Complete documentation suite for new developers

    - [ ] **Step 7.1: Testing Documentation Consolidation**
      - [x] **Task 7.1.1** ‚Äî Analyze all .md documents for testing-related content and create consolidation mapping
        - ‚úÖ Action: Go through all .md files to identify testing-related content
        - ‚úÖ Action: Create mapping of what should be merged vs archived
        - ‚úÖ Verify: Complete inventory of testing documentation across project

      - [x] **Task 7.1.2** ‚Äî Create detailed documentation consolidation plan with specific content to move
        - ‚úÖ Action: Plan specific sections to add to testing_strategy.md
        - ‚úÖ Action: Identify content to merge from each source document
        - ‚úÖ Verify: Clear plan for consolidation without execution

      - [x] **Task 7.1.3** ‚Äî Merge integration test patterns from tests/README_integration.md into testing_strategy.md
        - ‚úÖ Action: Add integration test organization section
        - ‚úÖ Action: Include test categories and service-specific tests
        - ‚úÖ Action: Add writing new integration tests guidance
        - ‚úÖ Verify: Integration patterns consolidated into single document

      - [x] **Task 7.1.4** ‚Äî Merge TEST_DOCKER environment guide from tests/TEST_DOCKER_GUIDE.md into testing_strategy.md
        - ‚úÖ Action: Add TEST_DOCKER environment variable section
        - ‚úÖ Action: Include service URLs for different environments
        - ‚úÖ Action: Add health check endpoints documentation
        - ‚úÖ Verify: Environment control consolidated

      - [x] **Task 7.1.5** ‚Äî Merge migration guide content from tests/MIGRATION_GUIDE.md into testing_strategy.md
        - ‚úÖ Action: Add migration patterns section
        - ‚úÖ Action: Include common migration scenarios
        - ‚úÖ Action: Document environment configuration migration
        - ‚úÖ Verify: Migration guidance integrated

      - [x] **Task 7.1.6** ‚Äî Merge testing workflow info from docs/ci-cd-release-management.md into testing_strategy.md
        - ‚úÖ Action: Add CI/CD testing workflows section
        - ‚úÖ Action: Include manual vs automated testing guidance
        - ‚úÖ Action: Document testing project scripts
        - ‚úÖ Verify: Workflow information consolidated

      - [x] **Task 7.1.7** ‚Äî Update docs/DEVELOPMENT.md to reference consolidated testing_strategy.md instead of individual guides
        - ‚úÖ Action: Replace individual testing doc references with consolidated reference
        - ‚úÖ Action: Update "More docs" section to point to testing_strategy.md
        - ‚úÖ Verify: Single reference to testing documentation

      - [x] **Task 7.1.8** ‚Äî Update documentation cross-references to point to consolidated testing_strategy.md
        - ‚úÖ Action: Find all references to individual testing docs
        - ‚úÖ Action: Update them to point to consolidated document
        - ‚úÖ Verify: No broken cross-references to old docs

      - [x] **Task 7.1.9** ‚Äî Remove or archive redundant testing documentation files after consolidation
        - ‚úÖ Action: Archive tests/TEST_DOCKER_GUIDE.md
        - ‚úÖ Action: Archive tests/MIGRATION_GUIDE.md
        - ‚úÖ Action: Consider archiving tests/README_integration.md
        - ‚úÖ Verify: Redundant docs removed, consolidated doc is single source

    - [ ] **Step 8: Validation and Cleanup**
      - Action: Run all integration tests with simplified system
      - Action: Remove legacy code and unused fixtures
      - Action: Ensure backward compatibility for essential features
      - Action: Performance validation (faster test startup, clearer errors)
      - Verify: All tests pass with improved developer experience

    **Success Criteria**:
    - ‚úÖ conftest.py reduced from 722 lines to < 200 lines
    - ‚úÖ Single configuration source (pyproject.toml [tool.integration])
    - ‚úÖ HTTP health checks using official endpoints
    - ‚úÖ pytest-native markers instead of custom decorators
    - ‚úÖ Environment detection via TEST_DOCKER environment variable
    - ‚úÖ Standard library TOML parsing with tomllib
    - ‚úÖ Clear, actionable error messages with user-focused guidance
    - ‚úÖ Focused mocking with pytest's monkeypatch
    - ‚úÖ Easy to understand and modify for new developers

    #### P2.3 ‚Äî Pre-commit Error Resolution Code Quality Improvements (PLANNED)

    **Goal**: Fix code organization issues introduced during pre-commit error resolution to follow established best practices and eliminate code duplication.

    **Issues Identified**:
    - **Code Duplication**: `scripts/check_integration_env.py` implements functions that should be imported from `conftest.py`
    - **Inconsistent Function Organization**: Utility functions scattered across modules instead of being centralized
    - **Maintenance Burden**: Multiple implementations of the same functionality increase maintenance overhead

    **Best Practices Analysis**:
    - ‚úÖ **Type Annotations**: Proper use of `Optional[dict[str, Any]]` for None parameters
    - ‚úÖ **Import Organization**: Correct import of `is_running_in_docker` from `backend.config`
    - ‚úÖ **Dependency Management**: Proper conditional dependency for `tomli` with Python version constraints
    - ‚úÖ **Import Error Handling**: Correct use of `# type: ignore[import-untyped]` for conditional imports
    - ‚ö†Ô∏è **Function Organization**: Local implementations instead of importing from appropriate modules

    **Implementation Plan**:

    - [ ] **Task P2.3.1** ‚Äî Consolidate Integration Test Utility Functions
      - Action: Move `get_available_services()`, `get_ollama_url()`, and `get_weaviate_hostname()` from `scripts/check_integration_env.py` to `tests/integration/conftest.py`
      - Action: Update `scripts/check_integration_env.py` to import these functions from `conftest.py`
      - Action: Ensure functions are properly exported and accessible
      - Verify: No code duplication, single source of truth for integration utilities

    - [ ] **Task P2.3.2** ‚Äî Improve Function Organization and Imports
      - Action: Review all utility functions in `scripts/` directory for proper module organization
      - Action: Identify functions that should be moved to appropriate `backend/` modules
      - Action: Update imports to use centralized utility functions
      - Verify: Clear separation of concerns and reduced code duplication

    - [ ] **Task P2.3.3** ‚Äî Add Integration Test Utility Function Documentation
      - Action: Add docstrings to utility functions in `conftest.py` explaining their purpose and usage
      - Action: Update `scripts/check_integration_env.py` documentation to reference imported functions
      - Action: Ensure clear documentation of the integration test utility API
      - Verify: Clear documentation for developers using these utilities

    **Success Criteria**:
    - ‚úÖ No code duplication between `scripts/` and `tests/integration/` modules
    - ‚úÖ Single source of truth for integration test utilities
    - ‚úÖ Clear function organization following established patterns
    - ‚úÖ Proper documentation of utility functions and their usage
    - ‚úÖ Maintained functionality while improving code organization

    **Expected Benefits**:
    - **Faster onboarding**: New developers understand the system quickly with pytest-native patterns
    - **Easier maintenance**: Less code to maintain and debug (80% reduction in conftest.py)
    - **Better reliability**: Fewer edge cases with complex caching/detection logic
    - **Clearer errors**: Simple messages that tell users exactly what to do (e.g., "Try: curl -i http://localhost:8080/v1/.well-known/ready")
    - **Modern practices**: Uses pytest's strengths (fixtures, markers, monkeypatch) instead of reimplementing them
    - **Performance**: No unnecessary caching/TTL logic for short-lived tests
    - **Standards compliance**: Follows 12-Factor config principles and pytest best practices

    **Risks to Monitor**:
    - ‚ö†Ô∏è Breaking changes for existing test patterns requiring migration
    - ‚ö†Ô∏è Loss of some advanced features (detailed error context, service caching)
    - ‚ö†Ô∏è Need for clear migration documentation
    - ‚ö†Ô∏è Potential initial test failures during transition


- [ ] **Core RAG Pipeline Components**
  - üîÑ Test retriever module with real models
  - üîÑ Test vectorization pipeline with real models
  - üîÑ Test reranking functionality with real models
  - üîÑ Test hybrid search logic with real models

- [ ] **Ollama Integration**
  - üîÑ Test Ollama client connectivity
  - üîÑ Test model availability checking
  - üîÑ Test model download via Ollama
  - üîÑ Test generation with real Ollama model

- [ ] **End-to-End QA Pipeline**
  - üîÑ Test complete QA workflow with mock services and real models
  - üîÑ Test error handling in QA pipeline
  - üîÑ Validate context retrieval and answer generation with real models
  - üîÑ Test different model configurations

- [ ] **Docker Environment**
  - üîÑ Test Docker build process
  - üîÑ Validate container startup
  - üîÑ Test service health checks
  - üîÑ Verify volume mounts work correctly

- [ ] **Real Model Operations**
  - üîÑ Test with actual embedding model (small/fast one)
  - üîÑ Test with actual reranker model (small/fast one)
  - üîÑ Validate model caching and reuse with real models
  - üîÑ Test model switching via environment variables

- [ ] **Performance & Memory**
  - üîÑ Test memory usage with real model loading
  - üîÑ Validate real model unloading/caching works
  - üîÑ Test concurrent real model access
  - üîÑ Monitor for memory leaks

- [ ] **Error Handling & Edge Cases**
  - üîÑ Test behavior with missing real models
  - üîÑ Test network failure scenarios
  - üîÑ Test disk space issues
  - üîÑ Test corrupted model files

- [ ] **Documentation & Scripts**
  - üîÑ Validate all scripts use correct imports
  - üîÑ Test docker-setup.sh with new configuration
  - üîÑ Update any outdated documentation
  - üîÑ Verify environment variable documentation

**Success Criteria**:
- ‚úÖ All unit tests pass
- ‚úÖ All integration tests pass
- ‚úÖ Core RAG functionality works end-to-end
- ‚úÖ Docker environment operates correctly
- ‚úÖ Real models load and function properly
- ‚úÖ No performance regressions
- ‚úÖ Error handling works as expected
- ‚úÖ Documentation is up-to-date

**Risks to Monitor**:
- ‚ö†Ô∏è Model loading performance impact
- ‚ö†Ô∏è Memory usage with multiple models
- ‚ö†Ô∏è Network dependency for model downloads
- ‚ö†Ô∏è Docker build time increases
- ‚ö†Ô∏è Test flakiness from real model operations


#### P3 ‚Äî Containerized CLI E2E copies (Partial Completion) ‚úÖ PARTIALLY COMPLETED

- **Why**: Host-run E2E miss packaging/runtime issues (entrypoint, PATH, env, OS libs). Twins validate the real image without replacing fast host tests.
- **Current Approach**: Used the existing `app` container which can run both Streamlit (web UI) and CLI commands via `docker compose exec app`. This leveraged the project's existing architecture where the `app` service is designed to handle multiple entry points.
- **Key Insight**: The project already supports CLI commands in the app container (see README.md: `./scripts/cli.sh python -m backend.qa_loop --question "What is in my docs?"`). This pattern was extended for automated testing rather than creating a separate `cli` service.
- **Benefits**: Simpler architecture, fewer services to maintain, aligns with existing project patterns, and leverages the same container that users interact with.

- [x] Step 1 ‚Äî Identify candidates ‚úÖ **COMPLETED**
  - Action: Listed E2E tests invoking CLI in-process (e.g., `backend.qa_loop`) such as `tests/e2e/test_qa_real_end_to_end.py`.
  - Verify: Confirmed they don't already run via container.

- [x] Step 2 ‚Äî Use existing app container for CLI testing ‚úÖ **COMPLETED**
  - Action: Leveraged the existing `app` service which can run both Streamlit and CLI commands via `docker compose exec`.
  - Verify: `docker compose exec app python -m backend.qa_loop --help` exited 0.

- [x] Step 3 ‚Äî Test helper ‚úÖ **COMPLETED**
  - Action: In `tests/e2e/conftest.py`, added `run_cli_in_container(args, env=None)` that uses `docker compose exec app ...`, returns `returncode/stdout/stderr`.
  - Verify: `--help` smoke passed.

- [x] Step 3.1 ‚Äî Review and validate implementation ‚úÖ **COMPLETED**
  - Action: Reviewed the implementation against best practices and simplified to use existing app container.
  - Verify: Confirmed that the simplified approach was correct and aligned with project structure.

- [x] Step 3.2 ‚Äî Clean up old complexity ‚úÖ **COMPLETED**
  - Action: Removed the separate `cli` service from `docker/docker-compose.yml` since we're using the existing `app` container.
  - Action: Updated `run_cli_in_container` fixture in `tests/e2e/conftest.py` to use `docker compose exec app`

- [x] Step 4 ‚Äî Readiness and URLs ‚úÖ **COMPLETED**
  - Action: Used existing `weaviate_compose_up`/`ollama_compose_up`; ensured ingestion uses compose-internal URLs.
  - Verify: Readiness checks passed before CLI twin runs.

- [x] Step 5 ‚Äî Create test twins ‚úÖ **COMPLETED**
  - Action: Added `_container_e2e.py` twins that call `run_cli_in_container([...])` with equivalent CLI subcommands; optionally marked with `@pytest.mark.docker`.
  - Verify: Single twin passed via `.venv/bin/python -m pytest -q tests/e2e/test_qa_real_end_to_end_container_e2e.py` after compose `--wait`.

- [ ] Step 6 ‚Äî Build outside tests (PENDING)
  - Action: Ensure scripts/CI build `kri-local-rag-app` once; helper should raise `pytest.UsageError` if image missing.
    - **Status**: Implemented.
      - `tests/e2e/conftest.py`: Modified `app_compose_up` fixture to check for `kri-local-rag-app:latest` image and raise `pytest.UsageError` if missing.
      - `scripts/build_app_if_missing.sh`: Created new script to build image only if missing.
      - `scripts/test_e2e.sh`: Updated to call `scripts/build_app_if_missing.sh` before running tests.
      - `docker/app.Dockerfile`: Fixed build issue by adding `COPY frontend/ /app/frontend/` before `pip install .` in the builder stage.
  - Verify: Second run is faster due to image reuse.
    - **Status**: Partially verified. The build process now correctly attempts to build the image if missing. However, tests are currently failing due to Weaviate-related issues.

**Current Blockers/Next Steps:**

- **Weaviate Connection/Schema Issues:**
  - `test_e2e_ingest_with_heavy_optimizations_into_real_weaviate` is failing with `weaviate.exceptions.WeaviateConnectionError: Connection to Weaviate failed. Details: [Errno 111] Connection refused`.
    - **Analysis**: This test uses `testcontainers` to spin up a Weaviate instance. The connection error suggests the container isn't fully ready or accessible.
    - **Action Taken**: Modified `tests/e2e/test_heavy_optimizations_weaviate_e2e.py` to specify the Weaviate image version as `cr.weaviate.io/semitechnologies/weaviate:1.32.0` (matching `docker-compose.yml`).
    - **Next Step**: Re-run tests to see if specifying the image version resolves the connection issue. If not, investigate further into `testcontainers` setup or Weaviate readiness checks.
  - `test_e2e_answer_with_real_services` is failing with `hybrid failed (Query call with protocol GRPC search failed with message could not find class Document in schema.); falling back to bm25`.
    - **Analysis**: This indicates the `Document` collection schema is not being created or is not accessible when the test runs.
    - **Next Step**: Investigate `backend/config.py` to confirm `COLLECTION_NAME` and verify that `ensure_weaviate_ready_and_populated()` is correctly creating the schema and populating data before the test.

- [ ] Step 7 ‚Äî Diagnostics and isolation (PENDING)
  - Action: On failure, print exit code, last 200 lines of app logs, and tails of `weaviate`/`ollama` logs; use ephemeral dirs/volumes.
  - Verify: Failures are actionable; runs are deterministic and isolated.

- [ ] Step 8 ‚Äî Wire into scripts/docs/CI (PENDING)
  - Action: Document commands in `docs/DEVELOPMENT.md` and `AI_instructions.md`; mention in `scripts/test.sh e2e` help; add a CI job for the containerized CLI subset.
  - Verify: Fresh env runs `tests/e2e/*_container_e2e.py` green; CI job passes locally under `act` and on hosted runners.

#### P5 ‚Äî E2E retrieval failure: QA test returns no context (Weaviate)

 - Context and goal
   - Failing test returns no context from Weaviate. Likely mismatch between collection name used by retrieval and the one populated by ingestion, or ingestion not executed.

 - [ ] **Task 1 ‚Äî Reproduce quickly**
   - **Action**: Run only the failing test to confirm the symptom (e.g., `pytest tests/e2e/test_qa_real_end_to_end.py`).
   - **Verify**: The test fails with an assertion related to empty context, confirming the issue is reproducible.

 - [ ] **Task 2 ‚Äî Check config and schema**
   - **Action**: Inspect `docker-compose.yml`, `.env` files, and test fixtures to find the `COLLECTION_NAME` being used. Connect to the Weaviate console and list collections.
   - **Verify**: The collection name used in the test exists in Weaviate, and its schema is as expected.

 - [ ] **Task 3 ‚Äî Confirm data population**
   - **Action**: Add a breakpoint or logging in the ingestion fixture (`tests/e2e/fixtures_ingestion.py`) to confirm it runs. Query the collection in Weaviate to count its objects.
   - **Verify**: The ingestion fixture executes successfully, and the target collection in Weaviate contains more than zero objects.

 - [ ] **Task 4 ‚Äî Probe retrieval directly**
   - **Action**: Add a temporary test case that directly calls the `retrieve_chunks` function against the populated collection.
   - **Verify**: The direct call to the retriever returns a non-empty list of documents, proving the retrieval logic is functional.

 - [ ] **Task 5 ‚Äî Standardize collection naming**
   - **Action**: Choose a single collection name for all E2E tests (e.g., `TestCollectionE2E`) and apply it consistently across tests, fixtures, and configurations.
   - **Verify**: A global search for the old collection name in the `tests/` directory yields no results.

 - [ ] **Task 6 ‚Äî Implement and verify**
   - **Action**: With the standardized name in place, re-run the full E2E test suite.
   - **Verify**: The originally failing QA test now passes successfully.

 - [ ] **Task 7 ‚Äî Add minimal guardrails**
   - **Action**: In the E2E setup fixture, add a log statement for the collection name being used. Create a new, small test that intentionally queries a non-existent collection.
   - **Verify**: The test logs show the correct collection name, and the new test confirms that querying an empty/non-existent collection returns an empty list rather than crashing.

#### P6 ‚Äî Minimalist Scripts Directory Cleanup

- **Goal**: Reorganize the `scripts/` directory for better clarity with minimal effort. Group related scripts into subdirectories. Avoid complex refactoring or new patterns like dispatchers.

- [ ] **Phase 1: Create Grouping Directories and a `common.sh`**
  - Action: Create the new directory structure: `scripts/test/`, `scripts/lint/`, `scripts/docker/`, `scripts/ci/`, `scripts/dev/`.
  - Action: Create a single `scripts/common.sh` file. Initially, it will only contain `set -euo pipefail` and basic color variables for logging.
  - Verify: The new directories and the `common.sh` file exist and are accessible.

#### P7 ‚Äî Torch.compile Optimization Debugging and Performance

- **Context**: During CLI usage, torch.compile optimization messages appear every time the script runs, and there's a suspicious debug message about "Skipping torch.compile optimization (tests or MagicMock instance)" appearing during normal app usage.

- **Root Cause Analysis**:
  - **torch.compile is not persistent**: Optimizations are lost when Python processes restart (expected behavior)
  - **CLI script starts new process**: Each `./scripts/cli.sh` run creates a fresh Python process, resetting global caches
  - **MagicMock detection issue**: Debug message suggests test-related mocking is active during normal app usage
  - **Environment configuration**: `.env` file had Docker service URLs instead of localhost URLs for local development

- **Current Status**: 
  - ‚úÖ Fixed `.env` configuration (localhost URLs for local CLI, Docker service URLs for containers)
  - ‚úÖ Reduced torch.compile verbosity to DEBUG level in `qa_loop.py`
  - ‚úÖ Added re-compilation prevention check within same process
  - üîç **PENDING**: Investigate why MagicMock detection triggers during normal app usage

- **Key Learnings so far**:
  - torch.compile optimizations are process-local and cannot be persisted across restarts
  - CLI script architecture (new process per run) inherently requires re-optimization
  - Test mocking infrastructure can leak into normal app usage if not properly isolated
  - Environment configuration needs to distinguish between local development and containerized usage

- [ ] **Task 1 ‚Äî Investigate MagicMock Detection in Normal Usage**
  - **Action**: Add detailed logging to `backend/retriever.py` to trace the exact condition that triggers the "Skipping torch.compile optimization (tests or MagicMock instance)" message.
  - **Action**: Check if any test configuration or environment variables are leaking into normal app usage.
  - **Verify**: The debug message only appears during actual test runs, not during normal CLI usage.

- [ ] **Task 2 ‚Äî Optimize torch.compile Application Strategy**
  - **Action**: Review if torch.compile should be applied to both embedding model and cross-encoder, or if one is sufficient.
  - **Action**: Consider adding environment variable to control torch.compile application (e.g., `TORCH_COMPILE_ENABLED=false` for development).
  - **Verify**: Performance is maintained while reducing unnecessary re-compilation overhead.

- [ ] **Task 3 ‚Äî Add Performance Monitoring**
  - **Action**: Add timing measurements around torch.compile operations to quantify the optimization overhead.
  - **Action**: Create a simple benchmark to measure the impact of torch.compile on inference speed.
  - **Verify**: Clear metrics showing the trade-off between compilation time and inference performance.

- [ ] **Task 4 ‚Äî Improve Error Handling and User Experience**
  - **Action**: Add more informative messages about torch.compile status (e.g., "Model optimization in progress..." with progress indicators).
  - **Action**: Consider caching compiled models to disk if possible to avoid re-compilation across process restarts.
  - **Verify**: Users understand what's happening during model optimization and the process feels responsive.
