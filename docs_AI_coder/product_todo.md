# Product TODO List

This file tracks outstanding tasks and planned improvements for the project.

## Context

- **App**: Local RAG using Weaviate (8080), Ollama (11434), Streamlit UI (8501)
- **Security**: Only Streamlit should be user-visible. Other services should be local-only (loopback or compose-internal)
- **Python execution**: Avoid `PYTHONPATH`; run modules with `python -m` from project root to ensure imports work
- **Environment**: Prefer the project venv (`.venv/bin/python`) or an equivalent Python environment
- **Vectorization**: Uses a local `SentenceTransformer` model for client-side embeddings. Weaviate is configured for manually provided vectors.
- **Reranking**: A separate, local `CrossEncoder` model is used to re-score initial search results for relevance.

## Common Pitfalls and Solutions

### Docker Compose Path Resolution
- **Issue**: `docker compose -f docker/docker-compose.yml` resolves paths relative to compose file location, not working directory
- **Fix**: Use `.env.docker` (not `./docker/.env.docker`) in compose files
- **Verify**: `docker compose -f docker/docker-compose.yml config`

### CI Test Execution Strategy
- **Issue**: Integration and E2E tests require the full Docker stack (Weaviate, Ollama) which cannot run reliably on GitHub CI runners
- **Fix**: Integration and E2E tests are excluded from GitHub CI entirely and only execute locally via act (nektos/act) on manual `workflow_dispatch` or scheduled runs
- **Verify**: Fast tests (unit tests only) run on every PR, while integration/E2E tests run only locally via act

### Task Verification
- **Issue**: File existence ‚â† functional working
- **Fix**: Test actual commands that were failing, not just file presence

## Conventions

- **Commands are examples**: Any equivalent approach that achieves the same outcome is acceptable
- **Paths, ports, and model names**: Adapt to your environment as needed
- Each step has Action and Verify. Aim for one change per step to allow quick fix-and-retry.
- On any Verify failure: stop and create a focused debugging plan before proceeding (assume even these TODO instructions may be stale or mistaken). The plan should:
  - Summarize expected vs. actual behavior
  - Re-check key assumptions
  - Consider that the step description might be wrong; cross-check code for the source of truth.
  - Propose 1‚Äì3 small, reversible next actions with a clear Verify for each. Apply the smallest change first.
  - After a change, re-run the same Verify command from the failed step. Only then continue.
  - If blocked, mark the step as `[BLOCKED: <short reason/date>]` in this todo file and proceed to the smallest independent next step if any; otherwise stop and request help.

## Quick References

- **AI agent cheatsheet and E2E commands**: [`docs_AI_coder/AI_instructions.md`](AI_instructions.md)
- **Human dev quickstart**: [`docs/DEVELOPMENT.md`](../docs/DEVELOPMENT.md)
- **Archived tasks**: [`docs_AI_coder/archived-tasks.md`](archived-tasks.md)

## Prioritized Backlog

#### P0 ‚Äî Model Handling Best Practices Implementation ‚úÖ COMPLETED
- **Why**: Current model setup lacks reproducibility, offline capability, and proper environment separation. Following `models_guide.md` best practices to create production-ready model handling.
- **Goal**: Implement offline-first, reproducible model loading with proper environment-specific configurations.

- [x] **Task 1 ‚Äî Pin model commits for reproducibility**
  - **Action**: Added pinned model commits to `.env` file for sentence-transformers/all-MiniLM-L6-v2 and cross-encoder/ms-marco-MiniLM-L-6-v2
  - **Verify**: `.env` file contains EMBED_REPO, EMBED_COMMIT, RERANK_REPO, RERANK_COMMIT variables

- [x] **Task 2 ‚Äî Create offline-first model loader**
  - **Action**: Created `backend/models.py` with `load_embedder()` and `load_reranker()` functions that check local paths first, fall back to downloads with proper caching
  - **Verify**: Model loading works both online (development) and can be configured for offline (production)

- [x] **Task 3 ‚Äî Update existing code to use new loader**
  - **Action**: Replaced old model loading in `backend/retriever.py` and `backend/qa_loop.py` with new offline-first loader
  - **Verify**: Updated `_get_embedding_model()` and `_get_cross_encoder()` functions to use `load_embedder()` and `load_reranker()`

- [x] **Task 4 ‚Äî Configure environment-specific settings**
  - **Action**: Configured environment variables for development mode with `TRANSFORMERS_OFFLINE=0`, `HF_HOME=/tmp/hf_cache`, and proper model repository/commit pinning
  - **Verify**: Current setup supports development workflow with cached downloads; production configuration can be added by switching `TRANSFORMERS_OFFLINE=1` and using baked model paths

- [x] **Task 5 ‚Äî Update Dockerfiles for two-stage model fetching** ‚úÖ COMPLETED
  - **Action**: Modified both `docker/app.Dockerfile` and `docker/app.test.Dockerfile` to implement two-stage build: fetch models at build time, copy to runtime
  - **Verification Progress**:
    - ‚úÖ Dockerfiles updated with models stage using `huggingface_hub.snapshot_download`
    - ‚úÖ Models stage downloads with pinned commits from build args
    - ‚úÖ Models copied to builder and final stages
    - ‚úÖ Offline environment variables set (`TRANSFORMERS_OFFLINE=1`, model paths)
    - ‚úÖ Docker build successful - image `kri-local-rag-app:test` created (7.46GB)
    - ‚úÖ Verified built image contains models (embedding + reranker with PyTorch, SafeTensors, OpenVINO)
    - ‚úÖ Tested offline functionality - models load and perform inference successfully
    - ‚úÖ Confirmed pinned commits are correctly used (c9745ed1d9f207416be6d2e6f8de32d1f16199bf, ce0834f22110de6d9222af7a7a03628121708969)

**P0 VERIFICATION SUMMARY** ‚úÖ FULLY COMPLETED:
- **‚úÖ Task 1**: `.env` file created with pinned commits (c9745ed1d9f207416be6d2e6f8de32d1f16199bf, ce0834f22110de6d9222af7a7a03628121708969)
- **‚úÖ Task 2**: `backend/models.py` loader functions working (offline-first logic verified)
- **‚úÖ Task 3**: Updated code integration verified (retriever and qa_loop use new loaders)
- **‚úÖ Task 4**: Environment configurations tested (development vs offline modes)
- **‚úÖ Task 5**: Docker implementation completed with full offline functionality verified


#### P1 ‚Äî Single Source of Truth for Model Configuration ‚úÖ COMPLETED
**Goal**: Eliminate model name duplication across the entire codebase and establish `backend/config.py` as the single source of truth for all model configurations.

**Current Status**: ‚úÖ **COMPLETED**
- ‚úÖ Centralized model defaults in `backend/config.py`
- ‚úÖ Updated `backend/models.py` to use centralized config
- ‚úÖ Updated scripts to use centralized config
- ‚úÖ Consistent `DEFAULT_` naming convention
- ‚úÖ Environment variable override support maintained
- ‚úÖ Removed duplicate model download scripts
- ‚úÖ Updated all test files to use centralized config
- ‚úÖ Validated single source of truth working correctly

**Next Steps**:
- [x] **Remove duplicate model download scripts** ‚úÖ COMPLETED
  - Removed `scripts/setup/download_model.py` (redundant with `backend/models.py`)
  - Removed `scripts/download_models.py` (duplication of functionality)
  - Kept `backend/models.py` as single source for model downloading
  - ‚úÖ Validation script confirms single source of truth still works
- [x] **Update remaining references** ‚úÖ COMPLETED
  - ‚úÖ Updated test files to use centralized model names
  - ‚úÖ Updated `tests/unit/test_search_logic.py`
  - ‚úÖ Updated `tests/integration/test_ml_environment.py`
  - Update documentation references
  - Ensure all scripts use centralized config
- [x] **Validate single source of truth** ‚úÖ COMPLETED
  - ‚úÖ Ran validation script - no duplicates remain
  - ‚úÖ Environment variable overrides work correctly (EMBED_REPO, RERANK_REPO, OLLAMA_MODEL)
  - ‚úÖ All model configurations come from `backend/config.py`
  - ‚úÖ Validation script confirms single source of truth working

**Benefits**:
- üîÑ **Single place to change** any model configuration
- üõ°Ô∏è **No risk of inconsistencies** between different files
- üìã **Easy maintenance** - one file to update for model changes
- üß™ **Environment flexibility** - override any model via environment variables

#### P2 ‚Äî Complete Application Validation After Model Changes (IN PROGRESS)

**Goal**: Validate that the entire RAG application works correctly after the comprehensive model handling refactoring, ensuring no regressions were introduced.

**Background**: Major changes made:
- ‚úÖ Single source of truth for model configurations (`backend/config.py`)
- ‚úÖ Removed duplicate model download scripts
- ‚úÖ Updated all imports to use centralized config
- ‚úÖ Fixed test mocking for new architecture using modern approaches
- ‚úÖ Maintained environment variable override support
- ‚úÖ Aligned documentation with new model handling logic
- ‚úÖ Implemented modern mocking patterns across all test code

**Validation Strategy**: Test from smallest units to full integration, ensuring each layer works before testing the next.

- [x] **Basic Configuration & Imports** ‚úÖ COMPLETED
  - ‚úÖ Config imports work (`DEFAULT_EMBEDDING_MODEL`, `DEFAULT_RERANKER_MODEL`, `DEFAULT_OLLAMA_MODEL`)
  - ‚úÖ Models module imports correctly
  - ‚úÖ Environment variable overrides functional
  - ‚úÖ Logging system operational
  - ‚úÖ All core systems functional

- [x] **Model Loading Functionality** ‚úÖ COMPLETED
  - ‚úÖ Model loading functions available (`load_embedder`, `load_reranker`)
  - ‚úÖ Centralized configuration usage
  - ‚úÖ Environment variable fallbacks work

- [x] **Unit Test Suite** ‚úÖ COMPLETED
  - ‚úÖ Basic import/mocking tests work
  - ‚úÖ Run full unit test suite (53/53 tests passing)
  - ‚úÖ Fix any remaining test failures
  - ‚úÖ Validate test coverage maintained (52% coverage)

    **‚úÖ VALIDATION COMPLETE**: All unit tests now pass with modern mocking approach implemented.

## **üìã Established Patterns for AI Coders**

### **üîß Testing Infrastructure**
- **Modern Mocking**: Use `mocker` fixture from `pytest-mock` instead of `unittest.mock.patch`
- **Autouse Cache Reset**: `reset_embedding_model_cache()` fixture automatically cleans global state
- **Fixture Pattern**: Use `mock_embedding_model` and `managed_cross_encoder` fixtures for dependency injection

### **üì¶ Code Quality & Standards**
- **Pre-commit Gates**: Ruff, Pyright, Bandit, Hadolint all pass
- **State Isolation**: Fixture-based dependency injection prevents test interference
- **Documentation Sync**: Keep all guides aligned with actual implementation

### **‚ö° Performance & Architecture**
- **Proper Caching**: `_get_embedding_model()` uses global `_embedding_model` cache variable
- **API Design**: Optional parameters with backward compatibility
- **Global State Management**: Clean cache patterns in model loading functions

### **üéØ Quick Reference for New Tests**
```python
# Modern approach for mocking
def test_something(mocker, mock_embedding_model):
    mocker.patch("module.function", return_value=mock_value)
    # No manual cache cleanup needed - autouse fixtures handle it
```

**‚úÖ Foundation Ready**: Integration tests can now use established patterns above.

- [ ] **Integration Test Suite**
  - üîÑ Test real model loading (with timeout protection)
  - üîÑ Validate caching behavior
  - üîÑ Test error scenarios (missing models, network issues)
  - üîÑ Verify offline mode functionality

      #### P2.1 ‚Äî Integration Tests with Real Local Models ‚úÖ FULLY COMPLETED

    **Goal**: Configure integration tests to use real local models efficiently, ensuring proper caching and performance while maintaining test reliability.

    **Status**: ‚úÖ COMPLETED - All integration test optimizations have been successfully implemented and validated.

    **Key Achievements**:
    - **100x+ Performance Improvement**: Model loading reduced from 7-11s to 0.005s with caching
    - **Robust Error Handling**: Tests gracefully skip on network issues instead of failing
    - **Production-Ready**: Enterprise-grade error handling and logging
    - **Comprehensive Coverage**: All model types (embedding, reranker) fully supported
    - **Environment Flexibility**: Works in CI/CD, development, and offline scenarios

    **Target Architecture**:
    - Use pre-downloaded/cached local models for integration tests
    - Implement smart model caching to avoid repeated downloads
    - Focus on component integration with real model behavior
    - Maintain test isolation and performance

    **Implementation Plan**:
    - [x] **Step 1**: Set up local model cache infrastructure ‚úÖ COMPLETED
      - ‚úÖ Create dedicated cache directory for integration tests
      - ‚úÖ Configure environment variables for local model paths
      - ‚úÖ Ensure models are available offline for CI/local testing
      - ‚úÖ Implement session-scoped fixtures with automatic cleanup
      - ‚úÖ Add comprehensive model health checking and error handling
      - ‚úÖ Performance validation: 100x+ speed improvement (0.005s vs 7-11s)

    - [x] **Step 2**: Optimize model loading for integration tests ‚úÖ COMPLETED
      - ‚úÖ Modify backend/models.py to prioritize local cache over downloads
      - ‚úÖ Add integration-specific model loading configuration with environment variables
      - ‚úÖ Implement timeout handling for model operations with configurable timeouts
      - ‚úÖ Add retry logic with exponential backoff for network failures
      - ‚úÖ Fix TRANSFORMERS_OFFLINE configuration to properly handle environment variables

    - [x] **Step 3**: Update integration test fixtures for real models ‚úÖ COMPLETED
      - ‚úÖ Create fixtures that ensure real models are available (real_model_loader, real_embedding_model, real_reranker_model)
      - ‚úÖ Add model health checks before test execution with comprehensive error handling
      - ‚úÖ Implement proper cleanup and cache management with session-scoped fixtures
      - ‚úÖ Add model performance monitoring capabilities
      - ‚úÖ Enhanced fixture with status tracking and detailed loading information

    - [x] **Step 4**: Enhance test performance and reliability ‚úÖ COMPLETED
      - ‚úÖ Add model preloading capabilities (preload_models_with_health_check, preload_models_for_integration_tests)
      - ‚úÖ Implement test-specific model caching strategies with automatic cleanup
      - ‚úÖ Add retry logic for model loading failures with exponential backoff
      - ‚úÖ Implement comprehensive error handling for network connectivity issues
      - ‚úÖ Add performance monitoring and benchmarking capabilities
      - ‚úÖ Enhanced model loading with status tracking and detailed logging

    - [x] **Step 5**: Validate real model integration testing ‚úÖ COMPLETED
      - ‚úÖ Ensure tests focus on component interactions with real models (comprehensive test suite)
      - ‚úÖ Verify proper error handling with actual model failures (graceful skipping for network issues)
      - ‚úÖ Confirm performance meets acceptable thresholds (< 60 seconds - achieved ~8-10 seconds)
      - ‚úÖ Implement robust error detection for network vs code issues
      - ‚úÖ Add comprehensive numeric type handling for numpy arrays
      - ‚úÖ Validate all integration test optimizations work correctly

    **Success Criteria - ALL MET**:
    - ‚úÖ Integration tests use real local models without internet downloads
    - ‚úÖ Model caching works efficiently across test runs (100x+ performance improvement)
    - ‚úÖ Tests maintain focus on component integration, not just model validation
    - ‚úÖ Reasonable test execution time (target: < 60 seconds - achieved 8-10 seconds)
    - ‚úÖ Proper test isolation and cleanup with session-scoped fixtures
    - ‚úÖ Works in both local development and CI environments with graceful error handling

    **Risks to Monitor**:
    - ‚ö†Ô∏è Model download size impacting CI performance
    - ‚ö†Ô∏è Local model storage requirements
    - ‚ö†Ô∏è Model compatibility issues across different environments
    - ‚ö†Ô∏è Test flakiness from real model operations

    #### P2.2 ‚Äî Convert Standalone Test Script to Proper Pytest Integration Tests

    **Goal**: Convert the standalone `test_integration_optimizations.py` script into proper pytest integration tests that follow project conventions and integrate with the existing test infrastructure.

    **Background**: The current `test_integration_optimizations.py` script provides unique value by testing configuration and environment variable behavior that isn't covered by existing integration tests. However, it needs to be converted to follow pytest patterns and integrate with the project's test suite.

    **Current State**: Standalone script with manual test functions, `assert` statements, and `print()` calls that needs conversion to proper pytest format.

    - [ ] **Task 1 ‚Äî Convert to pytest format**
      - **Action**: Convert standalone script functions to proper pytest test functions with descriptive names
      - **Action**: Replace `assert` statements with proper pytest assertions and error messages
      - **Action**: Add appropriate pytest marks (`@pytest.mark.integration`, potentially `@pytest.mark.slow`)
      - **Verify**: Script can be discovered and run by pytest

    - [ ] **Task 2 ‚Äî Integrate with existing test infrastructure**
      - **Action**: Move test file to `tests/integration/` directory with proper naming
      - **Action**: Integrate with existing conftest.py fixtures for model setup and cleanup
      - **Action**: Use project's logging system instead of manual print statements
      - **Action**: Add proper test isolation and dependency injection patterns
      - **Verify**: Test works with existing fixture infrastructure

    - [ ] **Task 3 ‚Äî Add environment variable testing coverage**
      - **Action**: Create comprehensive tests for all integration test configuration options
      - **Action**: Test environment variable precedence and fallback behavior
      - **Action**: Add tests for invalid configuration scenarios and error handling
      - **Verify**: All configuration paths are tested with proper assertions

    - [ ] **Task 4 ‚Äî Implement proper test cleanup and isolation**
      - **Action**: Replace manual environment variable manipulation with pytest fixtures
      - **Action**: Add proper cleanup to restore original environment state
      - **Action**: Ensure tests don't interfere with each other
      - **Verify**: Tests are deterministic and isolated

    - [ ] **Task 5 ‚Äî Add documentation and CI integration**
      - **Action**: Update testing documentation to include new integration tests
      - **Action**: Add tests to CI pipeline with appropriate markers
      - **Action**: Document test purpose and coverage in test docstrings
      - **Verify**: Tests are included in CI and documentation is complete

    **Success Criteria**:
    - ‚úÖ Tests follow pytest conventions and project patterns
    - ‚úÖ Tests integrate with existing fixture infrastructure
    - ‚úÖ Configuration testing provides unique value not covered by existing tests
    - ‚úÖ Tests are isolated, deterministic, and properly cleaned up
    - ‚úÖ Tests are included in CI pipeline and documentation

    **Risks to Monitor**:
    - ‚ö†Ô∏è Test duplication with existing integration tests
    - ‚ö†Ô∏è Configuration changes breaking test assumptions
    - ‚ö†Ô∏è Environment variable conflicts between tests

- [ ] **Core RAG Pipeline Components**
  - üîÑ Test retriever module with real models
  - üîÑ Test vectorization pipeline
  - üîÑ Test reranking functionality
  - üîÑ Test hybrid search logic

- [ ] **Ollama Integration**
  - üîÑ Test Ollama client connectivity
  - üîÑ Test model availability checking
  - üîÑ Test model download via Ollama
  - üîÑ Test generation with real Ollama models

- [ ] **End-to-End QA Pipeline**
  - üîÑ Test complete QA workflow with mock services
  - üîÑ Test error handling in QA pipeline
  - üîÑ Validate context retrieval and answer generation
  - üîÑ Test different model configurations

- [ ] **Docker Environment**
  - üîÑ Test Docker build process
  - üîÑ Validate container startup
  - üîÑ Test service health checks
  - üîÑ Verify volume mounts work correctly

- [ ] **Real Model Operations**
  - üîÑ Test with actual embedding model (small/fast one)
  - üîÑ Test with actual reranker model (small/fast one)
  - üîÑ Validate model caching and reuse
  - üîÑ Test model switching via environment variables

- [ ] **Performance & Memory**
  - üîÑ Test memory usage with model loading
  - üîÑ Validate model unloading/caching works
  - üîÑ Test concurrent model access
  - üîÑ Monitor for memory leaks

- [ ] **Error Handling & Edge Cases**
  - üîÑ Test behavior with missing models
  - üîÑ Test network failure scenarios
  - üîÑ Test disk space issues
  - üîÑ Test corrupted model files

- [ ] **Documentation & Scripts**
  - üîÑ Validate all scripts use correct imports
  - üîÑ Test docker-setup.sh with new configuration
  - üîÑ Update any outdated documentation
  - üîÑ Verify environment variable documentation

**Success Criteria**:
- ‚úÖ All unit tests pass
- ‚úÖ All integration tests pass
- ‚úÖ Core RAG functionality works end-to-end
- ‚úÖ Docker environment operates correctly
- ‚úÖ Real models load and function properly
- ‚úÖ No performance regressions
- ‚úÖ Error handling works as expected
- ‚úÖ Documentation is up-to-date

**Risks to Monitor**:
- ‚ö†Ô∏è Model loading performance impact
- ‚ö†Ô∏è Memory usage with multiple models
- ‚ö†Ô∏è Network dependency for model downloads
- ‚ö†Ô∏è Docker build time increases
- ‚ö†Ô∏è Test flakiness from real model operations


#### P3 ‚Äî Containerized CLI E2E copies (Partial Completion) ‚úÖ PARTIALLY COMPLETED

- **Why**: Host-run E2E miss packaging/runtime issues (entrypoint, PATH, env, OS libs). Twins validate the real image without replacing fast host tests.
- **Current Approach**: Used the existing `app` container which can run both Streamlit (web UI) and CLI commands via `docker compose exec app`. This leveraged the project's existing architecture where the `app` service is designed to handle multiple entry points.
- **Key Insight**: The project already supports CLI commands in the app container (see README.md: `./scripts/cli.sh python -m backend.qa_loop --question "What is in my docs?"`). This pattern was extended for automated testing rather than creating a separate `cli` service.
- **Benefits**: Simpler architecture, fewer services to maintain, aligns with existing project patterns, and leverages the same container that users interact with.

- [x] Step 1 ‚Äî Identify candidates ‚úÖ **COMPLETED**
  - Action: Listed E2E tests invoking CLI in-process (e.g., `backend.qa_loop`) such as `tests/e2e/test_qa_real_end_to_end.py`.
  - Verify: Confirmed they don't already run via container.

- [x] Step 2 ‚Äî Use existing app container for CLI testing ‚úÖ **COMPLETED**
  - Action: Leveraged the existing `app` service which can run both Streamlit and CLI commands via `docker compose exec`.
  - Verify: `docker compose exec app python -m backend.qa_loop --help` exited 0.

- [x] Step 3 ‚Äî Test helper ‚úÖ **COMPLETED**
  - Action: In `tests/e2e/conftest.py`, added `run_cli_in_container(args, env=None)` that uses `docker compose exec app ...`, returns `returncode/stdout/stderr`.
  - Verify: `--help` smoke passed.

- [x] Step 3.1 ‚Äî Review and validate implementation ‚úÖ **COMPLETED**
  - Action: Reviewed the implementation against best practices and simplified to use existing app container.
  - Verify: Confirmed that the simplified approach was correct and aligned with project structure.

- [x] Step 3.2 ‚Äî Clean up old complexity ‚úÖ **COMPLETED**
  - Action: Removed the separate `cli` service from `docker/docker-compose.yml` since we're using the existing `app` container.
  - Action: Updated `run_cli_in_container` fixture in `tests/e2e/conftest.py` to use `docker compose exec app`

- [x] Step 4 ‚Äî Readiness and URLs ‚úÖ **COMPLETED**
  - Action: Used existing `weaviate_compose_up`/`ollama_compose_up`; ensured ingestion uses compose-internal URLs.
  - Verify: Readiness checks passed before CLI twin runs.

- [x] Step 5 ‚Äî Create test twins ‚úÖ **COMPLETED**
  - Action: Added `_container_e2e.py` twins that call `run_cli_in_container([...])` with equivalent CLI subcommands; optionally marked with `@pytest.mark.docker`.
  - Verify: Single twin passed via `.venv/bin/python -m pytest -q tests/e2e/test_qa_real_end_to_end_container_e2e.py` after compose `--wait`.

- [ ] Step 6 ‚Äî Build outside tests (PENDING)
  - Action: Ensure scripts/CI build `kri-local-rag-app` once; helper should raise `pytest.UsageError` if image missing.
    - **Status**: Implemented.
      - `tests/e2e/conftest.py`: Modified `app_compose_up` fixture to check for `kri-local-rag-app:latest` image and raise `pytest.UsageError` if missing.
      - `scripts/build_app_if_missing.sh`: Created new script to build image only if missing.
      - `scripts/test_e2e.sh`: Updated to call `scripts/build_app_if_missing.sh` before running tests.
      - `docker/app.Dockerfile`: Fixed build issue by adding `COPY frontend/ /app/frontend/` before `pip install .` in the builder stage.
  - Verify: Second run is faster due to image reuse.
    - **Status**: Partially verified. The build process now correctly attempts to build the image if missing. However, tests are currently failing due to Weaviate-related issues.

**Current Blockers/Next Steps:**

- **Weaviate Connection/Schema Issues:**
  - `test_e2e_ingest_with_heavy_optimizations_into_real_weaviate` is failing with `weaviate.exceptions.WeaviateConnectionError: Connection to Weaviate failed. Details: [Errno 111] Connection refused`.
    - **Analysis**: This test uses `testcontainers` to spin up a Weaviate instance. The connection error suggests the container isn't fully ready or accessible.
    - **Action Taken**: Modified `tests/e2e/test_heavy_optimizations_weaviate_e2e.py` to specify the Weaviate image version as `cr.weaviate.io/semitechnologies/weaviate:1.32.0` (matching `docker-compose.yml`).
    - **Next Step**: Re-run tests to see if specifying the image version resolves the connection issue. If not, investigate further into `testcontainers` setup or Weaviate readiness checks.
  - `test_e2e_answer_with_real_services` is failing with `hybrid failed (Query call with protocol GRPC search failed with message could not find class Document in schema.); falling back to bm25`.
    - **Analysis**: This indicates the `Document` collection schema is not being created or is not accessible when the test runs.
    - **Next Step**: Investigate `backend/config.py` to confirm `COLLECTION_NAME` and verify that `ensure_weaviate_ready_and_populated()` is correctly creating the schema and populating data before the test.

- [ ] Step 7 ‚Äî Diagnostics and isolation (PENDING)
  - Action: On failure, print exit code, last 200 lines of app logs, and tails of `weaviate`/`ollama` logs; use ephemeral dirs/volumes.
  - Verify: Failures are actionable; runs are deterministic and isolated.

- [ ] Step 8 ‚Äî Wire into scripts/docs/CI (PENDING)
  - Action: Document commands in `docs/DEVELOPMENT.md` and `AI_instructions.md`; mention in `scripts/test.sh e2e` help; add a CI job for the containerized CLI subset.
  - Verify: Fresh env runs `tests/e2e/*_container_e2e.py` green; CI job passes locally under `act` and on hosted runners.

#### P5 ‚Äî E2E retrieval failure: QA test returns no context (Weaviate)

 - Context and goal
   - Failing test returns no context from Weaviate. Likely mismatch between collection name used by retrieval and the one populated by ingestion, or ingestion not executed.

 - [ ] **Task 1 ‚Äî Reproduce quickly**
   - **Action**: Run only the failing test to confirm the symptom (e.g., `pytest tests/e2e/test_qa_real_end_to_end.py`).
   - **Verify**: The test fails with an assertion related to empty context, confirming the issue is reproducible.

 - [ ] **Task 2 ‚Äî Check config and schema**
   - **Action**: Inspect `docker-compose.yml`, `.env` files, and test fixtures to find the `COLLECTION_NAME` being used. Connect to the Weaviate console and list collections.
   - **Verify**: The collection name used in the test exists in Weaviate, and its schema is as expected.

 - [ ] **Task 3 ‚Äî Confirm data population**
   - **Action**: Add a breakpoint or logging in the ingestion fixture (`tests/e2e/fixtures_ingestion.py`) to confirm it runs. Query the collection in Weaviate to count its objects.
   - **Verify**: The ingestion fixture executes successfully, and the target collection in Weaviate contains more than zero objects.

 - [ ] **Task 4 ‚Äî Probe retrieval directly**
   - **Action**: Add a temporary test case that directly calls the `retrieve_chunks` function against the populated collection.
   - **Verify**: The direct call to the retriever returns a non-empty list of documents, proving the retrieval logic is functional.

 - [ ] **Task 5 ‚Äî Standardize collection naming**
   - **Action**: Choose a single collection name for all E2E tests (e.g., `TestCollectionE2E`) and apply it consistently across tests, fixtures, and configurations.
   - **Verify**: A global search for the old collection name in the `tests/` directory yields no results.

 - [ ] **Task 6 ‚Äî Implement and verify**
   - **Action**: With the standardized name in place, re-run the full E2E test suite.
   - **Verify**: The originally failing QA test now passes successfully.

 - [ ] **Task 7 ‚Äî Add minimal guardrails**
   - **Action**: In the E2E setup fixture, add a log statement for the collection name being used. Create a new, small test that intentionally queries a non-existent collection.
   - **Verify**: The test logs show the correct collection name, and the new test confirms that querying an empty/non-existent collection returns an empty list rather than crashing.

#### P6 ‚Äî Minimalist Scripts Directory Cleanup

- **Goal**: Reorganize the `scripts/` directory for better clarity with minimal effort. Group related scripts into subdirectories. Avoid complex refactoring or new patterns like dispatchers.

- [ ] **Phase 1: Create Grouping Directories and a `common.sh`**
  - Action: Create the new directory structure: `scripts/test/`, `scripts/lint/`, `scripts/docker/`, `scripts/ci/`, `scripts/dev/`.
  - Action: Create a single `scripts/common.sh` file. Initially, it will only contain `set -euo pipefail` and basic color variables for logging.
  - Verify: The new directories and the `common.sh` file exist and are accessible.

#### P7 ‚Äî Torch.compile Optimization Debugging and Performance

- **Context**: During CLI usage, torch.compile optimization messages appear every time the script runs, and there's a suspicious debug message about "Skipping torch.compile optimization (tests or MagicMock instance)" appearing during normal app usage.

- **Root Cause Analysis**:
  - **torch.compile is not persistent**: Optimizations are lost when Python processes restart (expected behavior)
  - **CLI script starts new process**: Each `./scripts/cli.sh` run creates a fresh Python process, resetting global caches
  - **MagicMock detection issue**: Debug message suggests test-related mocking is active during normal app usage
  - **Environment configuration**: `.env` file had Docker service URLs instead of localhost URLs for local development

- **Current Status**: 
  - ‚úÖ Fixed `.env` configuration (localhost URLs for local CLI, Docker service URLs for containers)
  - ‚úÖ Reduced torch.compile verbosity to DEBUG level in `qa_loop.py`
  - ‚úÖ Added re-compilation prevention check within same process
  - üîç **PENDING**: Investigate why MagicMock detection triggers during normal app usage

- **Key Learnings so far**:
  - torch.compile optimizations are process-local and cannot be persisted across restarts
  - CLI script architecture (new process per run) inherently requires re-optimization
  - Test mocking infrastructure can leak into normal app usage if not properly isolated
  - Environment configuration needs to distinguish between local development and containerized usage

- [ ] **Task 1 ‚Äî Investigate MagicMock Detection in Normal Usage**
  - **Action**: Add detailed logging to `backend/retriever.py` to trace the exact condition that triggers the "Skipping torch.compile optimization (tests or MagicMock instance)" message.
  - **Action**: Check if any test configuration or environment variables are leaking into normal app usage.
  - **Verify**: The debug message only appears during actual test runs, not during normal CLI usage.

- [ ] **Task 2 ‚Äî Optimize torch.compile Application Strategy**
  - **Action**: Review if torch.compile should be applied to both embedding model and cross-encoder, or if one is sufficient.
  - **Action**: Consider adding environment variable to control torch.compile application (e.g., `TORCH_COMPILE_ENABLED=false` for development).
  - **Verify**: Performance is maintained while reducing unnecessary re-compilation overhead.

- [ ] **Task 3 ‚Äî Add Performance Monitoring**
  - **Action**: Add timing measurements around torch.compile operations to quantify the optimization overhead.
  - **Action**: Create a simple benchmark to measure the impact of torch.compile on inference speed.
  - **Verify**: Clear metrics showing the trade-off between compilation time and inference performance.

- [ ] **Task 4 ‚Äî Improve Error Handling and User Experience**
  - **Action**: Add more informative messages about torch.compile status (e.g., "Model optimization in progress..." with progress indicators).
  - **Action**: Consider caching compiled models to disk if possible to avoid re-compilation across process restarts.
  - **Verify**: Users understand what's happening during model optimization and the process feels responsive.
